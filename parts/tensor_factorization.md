## Tensor factorization

<!--
### Implementation notes:

* Start a new Python project `matchmaker-rescal`.
* Only command-line interface (~ minimum viable product)
* Produce results in EDN (<https://github.com/swaroopch/edn_format>).
* Consider also <https://github.com/nzhiltsov/Ext-RESCAL>?
* Learning to rank with <http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/>?
* Use <http://hyperopt.github.io/hyperopt> for hyper-parameter optimization?
-->

@Nickel2011, @Nickel2012, @Zhiltsov2013

<!--
### Tensor representation of RDF

Tensors provide modelling simplicity: multiple relations can be represented in a higher-order tensor.
Sparse adjacency matrices generated from RDF are often challenging to process.
Tensors are multidimensional arrays.
A natural way to represent multi-graphs is to use adjacency tensors.
Tensor order
RDF data can be represented as a third-order tensor.
Adjacency tensors
Slices: two-dimensional subarrays of tensors (i.e. matrices)
- Mode-3 slices are referred to as frontal slices (in a third-order tensor)
- Frontal slices of tensors correspond to adjacency matrices of given predicates.
Fibers: one-dimensional subarrays of tensors (i.e. vectors)
Tensor factorization
Latent factors

Two modes of a three-way tensors are formed by concatenating dataset's entities (subjects and objects).
Entities are not assumed to be homogeneous. They may instantiate different classes.
The third mode represents the relations (i.e. predicates).
$n \times n \time m$ tensor $\chi$. $n$ is the number of entities, $m$ is the number of relations.

Dealing with the noise in the data
Perspective of probabilistic databases

Hybrid approaches combining multiple methods
- E.g., re-ranking

Three-way tensor = third order tensor

Tensor factorization ~ tensor completion

TODO: Introduce modelling RDF data with three-way tensors. (With diagrams!)

Link prediction ranks entries in the reconstructed tensor by their values.

Can we say that tensors are a representational formalism for statistical relational learning?

Tensors balance expressiveness with complexity of their model.
Tensors can be considered multidimensional arrays.

Traditional methods for statistical relational learning, such as Markov logic networks, suffer from poor scalability.
Tensor factorization was shown to scale well.

No distinction between ontological and instance relations is maintained.
In this way, ontologies are handled as soft constraints [@Nickel2013a, p. 66].
The model of RDF data does not draw a distinction between terminological and instance data (TBox and ABox).
Both classes and instances are modelled as entities.
*"ontologies are handled like soft constraints, meaning that the additional information present in an ontology guides the factorization to semantically more reasonable results"* [@Nickel2012, p. 273]

*"local closed world assumption (LCWA), which is often used for training relational models"* [@Nickel2016, p. 13]
*"Training on all-positive data is tricky, because the model might easily over generalize."* [@Nickel2016, p. 24]
Negative examples can be generated by type constraints for predicates or valid ranges of literals.
@Nickel2016 proposes generating negative examples by "perturbing" true triples. (Basically, switching subjects in triples sharing the same predicate.) This generates "type-consistent" triples.
Switching objects in triples sharing the same predicate (under LCWA) is valid for functional properties.
@Nickel2016 proposes an approach that assumes the generated triples to be likely false.

Binary predicates in RDF ~ dyadic relations
-->

<!--
### Tensor factorization

Theoretical generalization of the abilities of tensor factorization is in @Nickel2013b.

Tensor factorization is inefficient if data contains a lot of strongly connected graph components.
Decomposition addresses high dimensionality and sparsity.

Alternative approach: Markov Random Fields (very flexible, but computationally expensive)
-->

<!--
### RESCAL

RESCAL was introduced in @Nickel2011.
It is a method based on factorization of a three-way tensor.
RESCAL does not require strict feature modelling.
RESCAL is a supervised machine learning method.
Unlike RDF, tensors can represent higher-order relations.
RESCAL exploits idiosyncratic properties of relational data.
It is able to use contextual data more distant in the relational graph. (=> collective learning)
RESCAL achieves leading performance for link prediction tasks.
RESCAL *"explains triples via pairwise interactions of latent features"* [@Nickel2016, p. 17]
RESCAL may be used to generate similarities between entities that may be then used in non-relational methods.
RESCAL was shown to be superior for link prediction tasks on two datasets. Nevertheless, what is the best machine learning method remains dataset-specific.
*"The main advantage of RESCAL, if compared to other tensor factorizations, is that it can exploit a collective learning effect when applied to relational data."* [@Nickel2012, p. 272]
@Nickel2012 shows how the execution of RESCAL can be parallelized and distributed across multiple computing nodes.
RESCAL adopts a closed world assumption: *"RESCAL approaches the problem of learning from positive examples only, by assuming that missing triples are very likely not true, an approach that makes sense in a high-dimensional but sparse domain."* [@Nickel2012, p. 273]
*"RESCAL can be regarded as a latent-variable model for multi-relational data"* [@Nickel2012, p. 273]
RESCAL is also fundamentally simpler than other tensor factorization methods. It is implemented in 120 lines of  code using standard Python/NumPy [@Nickel2011]. (It has low Kolmogorov complexity.)
Collective learning via latent components of the factorization.

**Comparison with matrix factorization**

RESCAL is similar to matrix factorization methods used in recommender systems [@Nickel2016, p. 18]
Matrix factorization (MF) offers good scalability, predictive accuracy, and modelling flexibility [@Koren2009, p. 44]
MF allows to incorporate both explicit and implicit feedback.
However, reshaping tensors into matrices causes data loss.

RESCAL uses unique latent representation of entities as subjects and objects, which enables efficient information propagation to capture correlations over relational chains [@Nickel2013c, p. 619].
-->

Time-aware link prediction [@Kuchar2016].
Weighting by time from award date to model decay of usefulness of older contract awards.

<!--
### Handling literals

The original version of RESCAL [@Nickel2011] ignores literals.
@Nickel2012 introduces an extension of RESCAL to handle literals.
Preprocessing literals: discretization of ordinal values, tokenization of plain texts, stemming words
NaÃ¯ve use of literals dramatically increases the dimensionality of the generated tensors.
Literals are included as entities, even though they never appear as subjects. This would make tensors even sparser.
@Nickel2012 proposes to handle literals by separate matrix factorization.
-->

<!--
### Feature selection

There is a need to balance expressiveness of the latent features with runtime of tensor factorization.

:awardedBidder (i.e. pc:awardedTender/pc:bidder, weighted by pc:awardDate)
pc:mainObject
pc:additionalObject
skos:closeMatch
skos:related
skos:broaderTransitive
rov:orgActivity

Experiments with YAGO in @Nickel2012 also include materialized `rdf:type` inferences.
- Should we do the same?

Alternative method for link prediction using tensor representation of RDF:
<http://semdeep.iiia.csic.es/files/SemDeep-17_paper_3.pdf>

*"tensor (and matrix) rank is a central parameter of factorization methods that determines generalization        ability as well as scalability"* [@Nickel2014].

There are 76 different relations in the Czech public procurement dataset.
More relations can be obtained from linked data.
However, we included only few relations in the tensor representation.
-->

<!--
### Learning to rank

Use a learning-to-rank method to optimize weights of features.
-->

### Benefits and drawbacks

### Matchmaking

### Implementation
